
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link
      href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900"
      rel="stylesheet"
    />

    <title>Nicholas Nisopoli</title>
<!--
Reflux Template
https://templatemo.com/tm-531-reflux
-->
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />

    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-style.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/lightbox.css" />
  </head>

  <body>
    <div id="page-wraper">
      <!-- Sidebar Menu -->
      <div class="responsive-nav">
        <i class="fa fa-bars" id="menu-toggle"></i>
        <div id="menu" class="menu">
          <i class="fa fa-times" id="menu-close"></i>
          <div class="container">
            <div class="image">
              <a href="#"><img src="assets/images/Nicholas.png" alt="" /></a>
            </div>
            <div class="author-content">
              <h1>Nicholas Nisopoli</h1>
              <p>
                Developer and AI Researcher
                <br>
                SurgiQ
              </p>

            </div>
            <nav class="main-nav" role="navigation">
              <ul class="main-menu">
                <li><a href="#section1">Home</a></li>
                <li><a href="#section2">Publications</a></li>
                <li><a href="#section3">Awards</a></li>
                <li><a href="#section4">Activities</a></li>
              </ul>
            </nav>
            <div class="social-network">
              <ul class="soial-icons">
                <li>
                  <a href="https://github.com/Nik-dot"  target="_blank"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span></a>
                </li>
                <li>
                  <a href="https://scholar.google.com/citations?user=gCG4cPYAAAAJ&hl=en"  target="_blank"><i class="icon scholar"></i><img src="assets/images/ClipartKey_1242841.png"/></a>
                </li>
                <li>
                  <a href="https://www.semanticscholar.org/author/Daya-Guo/51223794"  target="_blank"><i class="icon arxiv"><img src="assets/images/semanticscholar.ico"/></i></a>
                </li>
                 <li>
                  <a href="mailto:nicholas.nisopoli77@gmail.com"><i class="icon email"><img src="assets/images/email.png"/></i></a>
                  </li>
              </ul>
            </div>
            <div class="copyright-text">
              <p>Copyright 2019 Reflux Design</p>
            </div>
          </div>
        </div>
      </div>

      <section class="section home" data-section="section1">
        <div class="container">
          <div class="section-heading">
            <h2>About Me</h2>
            <div class="line-dec"></div>
          </div>

          <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Introduction
            </h4>    

          </div>   
            <p>
              I am a developer and AI researcher at SurgiQ, in Genova, Italy.
            </p>      
            <p>
              My work mainly focuses on developing Artificial Intelligence, based on logic and reasoning, capable of solving complex tasks for the healthcare sector, generating and managing efficient schedulings for both surgical and non-surgical patients.
            </p>   
           <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Educations
            </h4>   
            </div>
            
            <h3>
              <li>University of Genova</li>
            </h3>  
            <h8>
              Master's degree in Computer Engineering, 2019 - 2021).    
            </h8> 
            <br>
            <h8>
            Final Grade: 110/110 cum Laude
            </h8>
            <br>
            <h8>
            Thesis: <a href="assets/Tesi___Nicholas_Nisopoli.pdf">Artificial Intelligence techniques and applicative tools for hospital rehabilitation sessions rescheduling</a>
            </h8> 
             <h3>
              <li>University of Genova</li>
            </h3>  
            <h8>
              Bachelor's degree in Computer Engineering, 2016 - 2019.  
            </h8> 
            <br>
            <h8>
                Thesis: Platform Game development for Android devices in Unity environment
            </h8>
             
            <br>
            <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
              Experiences
            </h4>   
            </div>
              <h3>
              <li>Developer and AI Researcher - SurgiQ</li>
            </h3>  
            <h8>
              from December 2020 to Present.
            </h8>    
                       
             <h3>
              <li>Researcher - University of Genova</li>
            </h3>  
            <h8>
              Activity: Recognition of human activities using neural networks, from July 2021 to October 2021.
            </h8>             
                                                                   
       </section>

 
      <section class="section my-work" data-section="section2">
        <div class="container">
          <div class="section-heading">
            <h2>Publications</h2>
            <div class="line-dec"></div>
            <span
              >Below you can find highlighted publications and the full list of my publications.</span
            >
          </div>
          <div class="row">
            <div class="isotope-wrapper">
              <form class="isotope-toolbar">
                <label
                  ><input
                    type="radio"
                    data-type="h"
                    checked=""
                    name="isotope-filter"
                  />
                  <span>üî•Highlight</span></label
                >                    
                 <label
                  ><input
                    type="radio"
                    data-type="c"
                    name="isotope-filter"
                  />
                  <span>Conferences</span></label
                >     
                  <label
                  ><input
                    type="radio"
                    data-type="p"
                    name="isotope-filter"
                  />
                  <span>Preprints</span></label
                >                            
              </form>


               <div class="isotope-box">
                 
                <div class="isotope-item" data-type="c">
                  <h4 class="article-title mb-0 mt-0" itemprop="name">
                    2022
                  </h4>
                </div>  
                 
               <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Learning to Complete Code with Sketches 
                  </h3>
                  <h7 class="article-style" itemprop="articleBody">
                    ICLR 2022
                  </h7> 
                  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/pdf?id=q79uMSC6ZBT" rel="noopener">
                      PDF
                  </a>  
                  <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt, Miltiadis Allamanis
                  </h6>                   
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: Automatically generate (code) sketches, placing holes where ambiguity prevents us predicting terminal tokens.
                  </h5>
              
                </div>

               <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    UniXcoder: Unified Cross-Modal Pre-training for Code Representation
                  </h3>
                  <h7 class="article-style" itemprop="articleBody">
                    ACL 2022
                  </h7>  
                  <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin
                  </h6>                   
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: In this work, we present UniXcoder, a unified cross-modal pre-trained model for programming languages to support both code-related understanding and generation tasks. 
                  </h5>
              
                </div>                 
                 
               <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    ReACC: A Retrieval-Augmented Code Completion Framework
                  </h3>
                  <h7 class="article-style" itemprop="articleBody">
                    ACL 2022
                  </h7>  
                  <h6 class="article-style" itemprop="articleBody">
                    Shuai Lu, Nan Duan, Hojae Han, <strong><u>Daya Guo</u></strong>, seung-won hwang, Alexey Svyatkovskiy
                  </h6>                   
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: In this work, we propose ReACC, a retrieval-augmented code completion framework that utilizes external context for the code completion task by retrieving semantically and lexically similar codes from existing codebase.
                  </h5>
              
                </div>    
                 
               <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval
                  </h3>
                  <h7 class="article-style" itemprop="articleBody">
                    Findings of ACL 2022
                  </h7>  
                  <h6 class="article-style" itemprop="articleBody">
                    Canwen Xu, <strong><u>Daya Guo*</u></strong>, Nan Duan, Julian McAuley (*Equal Contributions)
                  </h6>                   
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for zero-shot text retrieval.
                  </h5>
              
                </div>                   
                 
                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://openreview.net/pdf?id=q79uMSC6ZBT" itemprop="url">Learning to Complete Code with Sketches</a>
                  </h3>
                  <h7 class="article-style" itemprop="articleBody">
                    ICLR 2022
                  </h7>                 
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: Automatically generate (code) sketches, placing holes where ambiguity prevents us predicting terminal tokens.
                  </h5>                
                </div>
                 
                <div class="isotope-item" data-type="c">
                  <h4 class="article-title" itemprop="name">
                    2021
                  </h4>
                </div>  

                <div class="isotope-item" data-type="p">
                  <h4 class="article-title mb-0 mt-0" itemprop="name">
                    2021
                  </h4>
                </div>  
                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    GraphCodeBERT: Pre-training Code Representations with Data Flow
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    ICLR 2021
                  </h7> 
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2009.08366.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/microsoft/CodeBERT" rel="noopener">
                      Code
                    </a>
                   <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang and Ming Zhou
                  </h6>       
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code, i.e. data flow, for pretraining.
                  </h5>
          

                </div>

                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://arxiv.org/pdf/2009.08366.pdf" itemprop="url">GraphCodeBERT: Pre-training Code Representations with Data Flow</a>
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    ICLR 2021
                  </h7> 
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code, i.e. data flow, for pretraining.
                  </h5>               

                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     NeurIPS 2021 Datasets and Benchmarks Track
                  </h7>    
                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://openreview.net/pdf?id=6lE4dQXaUcb" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/microsoft/CodeXGLUE" rel="noopener">
                      Dataset
                    </a>    
                   <h6 class="article-style" itemprop="articleBody">
                    Shuai Lu, <strong><u>Daya Guo*</u></strong>, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu   (*Equal Contributions)
                  </h6>                
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This paper introduces CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation that includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison.
                  </h5>                                   
   
                </div>

                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://openreview.net/pdf?id=6lE4dQXaUcb" itemprop="url">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</a>
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     NeurIPS 2021 Datasets and Benchmarks Track
                  </h7>    
              
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This paper introduces CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation that includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison.
                  </h5>                                       
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Multi-modal Representation Learning for Video Advertisement Content Structuring
                  </h3>
                  
                   <h7 class="article-style" itemprop="articleBody">
                     ACM Multimedia 2021
                  </h7>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2109.06637.pdf" rel="noopener">
                      PDF
                    </a>                  
                   <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Zhaoyang Zeng
                  </h6>                     
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: In this paper, we propose a multi-modal encoder to learn multi-modal representation from video advertisements by interacting between video-audio and text. The framework achieves the 1st place on the task of Multi-modal Ads Video Understanding in ACM Multimedia 2021 Grand Challenge.
                  </h5>                                   
    
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Syntax-Enhanced Pre-trained Model
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     ACL 2021
                  </h7>    
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2012.14116.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/Hi-ZenanXu/Syntax-Enhanced_Pre-trained_Model" rel="noopener">
                      Dataset
                    </a>   
                    <h6 class="article-style" itemprop="articleBody">
                    Zenan Xu, <strong><u>Daya Guo</u></strong>, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun Quan, Nan Duan and Daxin Jiang
                  </h6>                                      
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: We present a model that utilizes the syntax of text, i.e. dependency tree, in both pre-training and fine-tuning stages. 
                  </h5>                                   
  
                </div>





                <div class="isotope-item" data-type="p">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    AR-LSAT: Investigating Analytical Reasoning of Text
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    Arxiv 2021
                  </h7> 
                  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2104.06598.pdf" rel="noopener">
                    PDF
                  </a>
                  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/zhongwanjun/AR-LSAT" rel="noopener">
                    Code and Dataset
                  </a>    
                   <h6 class="article-style" itemprop="articleBody">
                    Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, <strong><u>Daya Guo</u></strong>, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou and Nan Duan.
                  </h6>                                    
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This paper studies the challenge of analytical reasoning of text and introduces a new dataset consisting of questions from the Law School Admission Test from 1991 to 2016, and designs two different baselines which struggle to solve this task. 
                  </h5>
               
                </div>

                <div class="isotope-item" data-type="c">
                  <h4 class="article-title" itemprop="name">
                    2020
                  </h4>
                </div>  

                <div class="isotope-item" data-type="p">
                  <h4 class="article-title" itemprop="name">
                    2020
                  </h4>
                </div> 

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    ACL 2020
                  </h7> 
                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aclweb.org/anthology/2020.acl-main.544.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/microsoft/EA-VQ-VAE" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="assets/doc/evidence-aware-inferential-text-generation-with-vector-quantised-variational-autoencoder.pdf" rel="noopener">
                      Slide
                    </a>          
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://drive.google.com/file/d/17Nc8Pz1u7Ha5uIpLjFQikWahIMmfTDof/view?usp=sharing" rel="noopener">
                      Video
                    </a>      
                   <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang and Ming Zhou.
                  </h6>                                   
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: An approach equipped with a Vector Quantised-Variational Autoencoder that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts, which provides state-of-the-art performance on both Event2Mind and ATOMIC datasets.
                  </h5>
               
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    CodeBERT: A Pre-Trained Model for Programming and Natural Languages
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     Findings of EMNLP 2020
                  </h7> 
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2002.08155.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/microsoft/CodeBERT" rel="noopener">
                      Code
                    </a>          
                   <h6 class="article-style" itemprop="articleBody">
                    Zhangyin Feng, <strong><u>Daya Guo</u></strong>, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
                  </h6>                                  
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work develops CodeBERT with Transformer-based neural architecture, and trains it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators.
                  </h5>                                   
  
                </div>

                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://arxiv.org/pdf/2002.08155.pdf" itemprop="url">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a>
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     Findings of EMNLP 2020
                  </h7>                    
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work develops CodeBERT with Transformer-based neural architecture, and trains it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators.
                  </h5>                                       
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     AAAI  2020
                  </h7>    
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1909.05311.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/DecstionBack/AAAI_2020_CommonsenseQA" rel="noopener">
                      Code
                    </a>  
                   <h6 class="article-style" itemprop="articleBody">
                    Shangwen Lv, <strong><u>Daya Guo*</u></strong>, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, and Songlin Hu   (*Equal Contributions)
                  </h6>                                         
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work proposes to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence, and achieves the state-of-the-art accuracy on the CommonsenseQA leaderboard.
                  </h5>                                   

                </div>

                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://arxiv.org/pdf/1909.05311.pdf" itemprop="url"> Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering</a>
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     AAAI  2020
                  </h7>                    
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work proposes to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence, and achieves the state-of-the-art accuracy on the CommonsenseQA leaderboard.
                  </h5>                                       
                </div>

                <div class="isotope-item" data-type="p">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    CodeBLEU: a Method for Automatic Evaluation of Code Synthesis
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    Arxiv 2020
                  </h7> 
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2009.10297.pdf" rel="noopener">
                      PDF
                    </a> 
                   <h6 class="article-style" itemprop="articleBody">
                    Shuo Ren, <strong><u>Daya Guo</u></strong>, Shuai Lu, Long Zhou,Shujie Liu, Duyu Tang, Ming Zhou, Ambrosio Blanco, Shuai Ma
                  </h6>                                           
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work introduces a new automatic evaluation metric, dubbed CodeBLEU, which absorbs the strength of BLEU in the n-gram match and further injects code syntax via abstract syntax trees (AST) and code semantics via data-flow and can achieve a better correlation with programmer assigned scores compared with BLEu and accuracy. 
                  </h5>
                         
                </div>

                <div class="isotope-item" data-type="p">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Inferential Text Generation with Multiple Knowledge Sources and Meta-Learning
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                    Arxiv 2020
                  </h7> 
                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2004.03070.pdf" rel="noopener">
                      PDF
                    </a>  
                  <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Akari Asai, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Jian Yin, Ming Zhou
                  </h6>                                     
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work uses not only structured commonsense knowledge bases, but also natural language snippets from search-engine results incorporated into a generative base model via key-value memory network and introduces a meta-learning based multi-task learning algorithm.
                  </h5>
                                 
                </div>

                <div class="isotope-item" data-type="c">
                  <h4 class="article-title" itemprop="name">
                    2019
                  </h4>
                </div>  

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     ACL 2019
                  </h7>   
                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/1906.07108.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="assets/doc/daya-acl2019-slides.pdf" rel="noopener">
                      Slide
                    </a> 
                   <h6 class="article-style" itemprop="articleBody">
                    <strong><u>Daya Guo</u></strong>, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin
                  </h6>                                      
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: An approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment, and shows that both the context-aware retriever and the meta-learning strategy improve accuracy.
                  </h5>                                   
     
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     EMNLP 2019
                  </h7> 
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aclweb.org/anthology/D19-1248.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/taoshen58/MaSP" rel="noopener">
                      Code
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://drive.google.com/file/d/1Y9XrTdEaOFaFjUxEA4THWL0VhT4sBXXN/view" rel="noopener">
                      Video
                    </a>     
                    <h6 class="article-style" itemprop="articleBody">
                    Tao Shen, Xiubo Geng, Tao Qin, <strong><u>Daya Guo</u></strong>, Duyu Tang, Nan Duan, Guodong Long, and Daxin Jiang
                  </h6>                                       
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This work proposes an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model.
                  </h5>                                   
     
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Multi-modal Representation Learning for Short Video Understanding and Recommendation
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     ICMEW 2019
                  </h7>    
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8795067" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/guoday/ICME2019-CTR" rel="noopener">
                      Code
                    </a>   
                   <h6 class="article-style" itemprop="articleBody">
                   <strong><u>Daya Guo</u></strong>, Jiangshui Hong, Binli Luo, Qirui Yan, and Zhangming Niu
                  </h6>                                        
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: A multi-modal representation learning method to improve the performance of recommender systems and a novel Key-Value Memory to map dense real-values into vectors, which could obtain more sufficient semantic in a nonlinear manner.
                  </h5>                                   
    
                </div>

                <div class="isotope-item" data-type="c">
                  <h4 class="article-title" itemprop="name">
                    2018
                  </h4>
                </div>  

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     NeurIPS 2018
                  </h7>   
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/guoday/Dialog-to-Action" rel="noopener">
                      Code
                    </a>  
                    <h6 class="article-style" itemprop="articleBody">
                   <strong><u>Daya Guo</u></strong>, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin
                  </h6>                                           
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: An approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base, and shows that the semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.
                  </h5>                                   

                </div>

                <div class="isotope-item" data-type="h">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    <a href="https://papers.nips.cc/paper/7558-dialog-to-action-conversational-question-answering-over-a-large-scale-knowledge-base.pdf" itemprop="url"> Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base</a>
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     NeurIPS 2018
                  </h7>                        
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: An approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base, and shows that the semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.
                  </h5>                                         
                </div>

                <div class="isotope-item" data-type="c">
                  <h3 class="article-title mb-0 mt-0" itemprop="name">
                    Question Generation from SQL Queries Improves Neural Semantic Parsing
                  </h3>
                   <h7 class="article-style" itemprop="articleBody">
                     EMNLP 2018
                  </h7> 
                     <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.aclweb.org/anthology/D18-1188.pdf" rel="noopener">
                      PDF
                    </a>
                    <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/guoday/Question-Generation-VAE" rel="noopener">
                      Code
                    </a>    
                    <h6 class="article-style" itemprop="articleBody">
                   <strong><u>Daya Guo</u></strong>, Yibo Sun, Duyu Tang, Nan Duan, Jian Yin, Hong Chi, James Cao, Peng Chen, and Ming Zhou
                  </h6>                                        
                  <h5 class="article-style" itemprop="articleBody">
                    TLDR: This study conducts a study on WikiSQL, the largest hand-annotated semantic parsing dataset to date, and demonstrates that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data.
                  </h5>                                   
   
                </div>

               </div>
            </div>
          </div>
      </section>
     <section class="section my-services" data-section="section3">
        <div class="container">
          <div class="section-heading">
            <h2>Awards</h2>
            <div class="line-dec"></div>
          </div>

           <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Data Mining Competition Awards
            </h4>   
            </div>
          

            <h9>
            <li style="margin-top: 10px;">2nd Place Award of 2021 Tencent QQ Brower Competition & ACM CIKM 2021 AnalyticCup [<a href="assets/doc/Multi_modal_Representation_Pre_training_for_Video_Retrieval.pdf"><font color="blue">report</font></a>] 
            </li>
            </h9>  
          
            <h9>
            <li style="margin-top: 10px;">1st Place Award of 2021 Tencent Advertising Algorithm Competition & ACM Multimedia Grand Challenge [<a href="https://mp.weixin.qq.com/s/q1ZK2rqBJeyYgRzXmoHbbQ"><font color="blue">news</font></a>] 
            </li>
            </h9>          
          
            <h9>
            <li style="margin-top: 10px;">1st Place Award of 2020 Tencent Advertising Algorithm Competition [<a href="https://github.com/guoday/Tencent2020_Rank1st"><font color="blue">code</font></a>] [<a href="https://mp.weixin.qq.com/s/jG22G0CTRsS09MYWy7PJFg"><font color="blue">news</font></a>] 
            </li>
            </h9>

            <h9>
            <li style="margin-top: 10px;">1st Place Award of 2019 Tencent Advertising Algorithm Competition [<a href="https://github.com/guoday/Tencent2019_Preliminary_Rank1st"><font color="blue">code</font></a>] [<a href="https://cloud.tencent.com/developer/article/1461354"><font color="blue">news</font></a>]
            </li>
            </h9>

            <h9>
            <li style="margin-top: 10px;">2nd Place Award of 2018 CCF Big Data and Computing Competition [<a href="https://github.com/PandasCute/2018-CCF-BDCI-China-Unicom-Research-Institute-top2"><font color="blue">code</font></a>]
            </li>
            </h9>

            <h9>
            <li style="margin-top: 10px;">6th Place Award of ICME 2019 & ByteDance Grand Challenge  [<a href="https://github.com/guoday/ICME2019-CTR"><font color="blue">code</font></a>]  [<a href="https://ieeexplore.ieee.org/document/8795067"><font color="blue">paper</font></a>]
            </li>
            </h9>

            <br>
           <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Academic Competition Awards
            </h4>   
            </div>

             <h9>
            <li style="margin-top: 10px;">Meritorious Winner of 2017 Mathematical Contest in Modeling
            </li>
            </h9>

             <h9>
            <li style="margin-top: 10px;">Second Prize of 2017 Guangdong Collegiate Programming Contest
            </li>
            </h9>

          <br>
           <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Scholarship
            </h4>   
            </div>
            <h9>
            <li style="margin-top: 10px;">Microsoft Research Asia (MSRA) Fellowship, 2020   [<a href="https://www.microsoft.com/en-us/research/academic-program/fellowships-microsoft-research-asia/#!fellows"><font color="blue">news</font></a>]     
            </li>
            </h9>
            <h8>
              12 outstanding Ph.D. students in the Asia-Pacific region
            </h8> 

            <h9>
            <li style="margin-top: 10px;">Sensetime Scholarship, 2017  
            </li>
            </h9>
            <h8>
              24 outstanding undergraduate students in China
            </h8>  
                       
            <br>
            <h9>
            <li style="margin-top: 10px;">National Scholarship   
            </li>
            </h9>
            <h8>
              2015, 2016 and 2020 in Sun Yat-sen University
            </h8>
           

      </section>

      <section class="section contact-me" data-section="section4">
        <div class="container">
          <div class="section-heading">
            <h2>Activities</h2>
            <div class="line-dec"></div>
          </div>

            <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            English
            </h4>  
            </div>
            <h9>
            <li style="margin-top: 10px;">Conference Reviewer: ACL; EMNLP; NAACL; NLPCC</li>
            </h9>
            <h9>
            <li style="margin-top: 10px;">HuggingFace Contributions: <a href="https://huggingface.co/microsoft/codebert-base"><font color="blue">codebert-base</font></a>; <a href="https://huggingface.co/microsoft/graphcodebert-base"><font color="blue">graphcodebert-base</font></a> </li>
            </h9>
            <h9>
            <li style="margin-top: 10px;">CodeXGLUE: <a href="https://www.microsoft.com/en-us/research/blog/codexglue-a-benchmark-dataset-and-open-challenge-for-code-intelligence/"><font color="blue">A benchmark dataset and open challenge for code intelligence</font></a> </li>
            </h9>

            <br>
            <div class="isotope-item">
            <h4 class="article-title mb-0 mt-0" itemprop="name">
            Chinese
            </h4>  
            </div>

            <h9>
            <li style="margin-top: 10px;"><a href="https://www.msra.cn/zh-cn/news/features/codexglue"><font color="blue">‰ª£Á†ÅÊô∫ËÉΩÊñ∞Âü∫ÂáÜÊï∞ÊçÆÈõÜCodeXGLUEÊù•Ë¢≠ÔºåÂ§öËßíÂ∫¶Ë°°ÈáèÊ®°Âûã‰ºòÂä£</font></a> </li>
            </h9>
            <h9>
            <li style="margin-top: 10px;"><a href="https://mp.weixin.qq.com/s/q1ZK2rqBJeyYgRzXmoHbbQ"><font color="blue">2021Âπ¥ËÖæËÆØÂπøÂëäÁÆóÊ≥ïÂ§ßËµõÂÜ†ÂÜõ</font></a> <br></li>
            </h9>          
            <h9>
            <li style="margin-top: 10px;"><a href="https://cloud.tencent.com/developer/article/1675513"><font color="blue">2020Âπ¥ËÖæËÆØÂπøÂëäÁÆóÊ≥ïÂ§ßËµõÂÜ†ÂÜõ</font></a> <br></li>
            </h9>
            <h9>
            <li style="margin-top: 10px;"><a href="https://cloud.tencent.com/developer/article/1461354"><font color="blue">2019Âπ¥ËÖæËÆØÂπøÂëäÁÆóÊ≥ïÂ§ßËµõÂÜ†ÂÜõ</font></a> <br></li> 
            </h9>
            <h9>
            <li style="margin-top: 10px;"><a href="https://tech.sina.com.cn/roll/2020-02-21/doc-iimxxstf3359549.shtml"><font color="blue">CodeBERT: Èù¢ÂêëÁºñÁ®ãËØ≠Ë®ÄÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã</font></a> <br></li>
            </h9>
            <h9>
            <li style="margin-top: 10px;"><a href="https://www.msra.cn/zh-cn/news/features/machine-reasoning-conversational-qa-semantic-parsing"><font color="blue">Âü∫‰∫éÊé®ÁêÜÁöÑÂ§öËΩÆËØ≠‰πâÂàÜÊûêÂíåÈóÆÁ≠î</font></a> <br></li>
            </h9>
            <h9>
            <li style="margin-top: 10px;"><a href="https://www.msra.cn/zh-cn/news/features/machine-reasoning-for-commonsense-question-answering"><font color="blue">Êú∫Âô®Êé®ÁêÜÂú®Â∏∏ËØÜÈóÆÁ≠î‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®</font></a><br></li>
            </h9>
            
       </section>
    </div>

    <!-- Scripts -->
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <script src="assets/js/isotope.min.js"></script>
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/lightbox.js"></script>
    <script src="assets/js/custom.js"></script>
    <script>
      //according to loftblog tut
      $(".main-menu li:first").addClass("active");

      var showSection = function showSection(section, isAnimate) {
        var direction = section.replace(/#/, ""),
          reqSection = $(".section").filter(
            '[data-section="' + direction + '"]'
          ),
          reqSectionPos = reqSection.offset().top - 0;

        if (isAnimate) {
          $("body, html").animate(
            {
              scrollTop: reqSectionPos
            },
            800
          );
        } else {
          $("body, html").scrollTop(reqSectionPos);
        }
      };

      var checkSection = function checkSection() {
        $(".section").each(function() {
          var $this = $(this),
            topEdge = $this.offset().top - 80,
            bottomEdge = topEdge + $this.height(),
            wScroll = $(window).scrollTop();
          if (topEdge < wScroll && bottomEdge > wScroll) {
            var currentId = $this.data("section"),
              reqLink = $("a").filter("[href*=\\#" + currentId + "]");
            reqLink
              .closest("li")
              .addClass("active")
              .siblings()
              .removeClass("active");
          }
        });
      };

      $(".main-menu").on("click", "a", function(e) {
        e.preventDefault();
        showSection($(this).attr("href"), true);
      });

      $(window).scroll(function() {
        checkSection();
      });
    </script>
  </body>
</html>
